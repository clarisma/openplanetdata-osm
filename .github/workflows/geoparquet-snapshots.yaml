name: Build & Publish Geoparquet Snapshots

on:
  workflow_dispatch:
  workflow_run:
    workflows: ["Build & Publish PBF Snapshots"]
    types: [completed]

env:
  REMOTE_NAME: openplanetdata-r2

jobs:
  geoparquet:
    runs-on: openplanetdata-cortex

    outputs:
      tag: ${{ steps.date.outputs.tag }}

    steps:
      - name: Set date tag
        id: date
        run: echo "tag=$(date '+%Y%m%d')" >>"$GITHUB_OUTPUT"

      - name: Install system dependencies
        shell: bash
        run: |
          need_install() { ! command -v "$1" &>/dev/null; }
          pkgs=()
          need_install jq    && pkgs+=(jq)
          need_install unzip && pkgs+=(unzip)
          if [ ${#pkgs[@]} -gt 0 ]; then
            sudo dnf -y install "${pkgs[@]}"
          fi

      - uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: 21

      - uses: actions/setup-python@v5
        with: {cache: "pip"}

      - name: Python deps for merge
        run: pip install --quiet duckdb

      - name: Clone ohsome @ latest tag & build JAR
        id: build_ohsome
        run: |
          TAG=$(curl -s https://api.github.com/repos/GIScience/ohsome-planet/releases/latest | jq -r .tag_name)
          git clone --depth 1 --branch "$TAG" --recurse-submodules https://github.com/GIScience/ohsome-planet.git
          cd ohsome-planet
          ./mvnw -q clean package -DskipTests
          JAR=$(ls ohsome-planet-cli/target/ohsome-planet*.jar | head -n1)
          echo "JAR_PATH=$(pwd)/$JAR" >>"$GITHUB_ENV"

      - name: Configure rclone
        env:
          RCLONE_CONFIG_DATA: ${{ secrets.RCLONE_CONFIG_DATA }}
        run: |
          mkdir -p ~/.config/rclone
          printf '%s' "$RCLONE_CONFIG_DATA" > ~/.config/rclone/rclone.conf
          command -v rclone >/dev/null || curl https://rclone.org/install.sh | sudo bash

      - name: Download latest published PBF
        run: |
          rclone copy \
            --http-url https://download.openplanetdata.com :http:osm/planet/pbf/planet-latest.osm.pbf . \
            --multi-thread-cutoff 0 --multi-thread-streams 128 --multi-thread-chunk-size 512M \
            --transfers 1 --progress
          mv planet-latest.osm.pbf "planet-${{ steps.date.outputs.tag }}.osm.pbf"

      - name: Build GeoParquet with ohsome-planet from PBF
        env:
          TAG: ${{ steps.date.outputs.tag }}
        run: |
          PBF="planet-${TAG}.osm.pbf"
          OUT_DIR="ohsome-${TAG}"
          time java -Xmx32g -jar "$JAR_PATH" contributions \
               --pbf "$PBF" --output "$OUT_DIR" --overwrite   # produces parquet partitions
          # merge "latest" partitions to single legacy file
          time python - <<'PY'
          import duckdb, os, glob, os.path as p
          tag = os.environ["TAG"]
          glob_pat = f"ohsome-{tag}/contributions/latest/*.parquet"
          duckdb.sql(f"COPY (SELECT * FROM parquet_scan('{glob_pat}')) "
          f"TO 'planet-{tag}.osm.parquet' "
          " (FORMAT PARQUET, COMPRESSION ZSTD);")
          PY

      - name: Generate checksums & metadata
        env:
          TAG: ${{ steps.date.outputs.tag }}
        run: |
          f="planet-${TAG}.osm.parquet"
          sha256sum "$f" >"$f.sha256"
          stat -c '%s %W %Z' "$f" | \
            awk '{print "{\"created\":"($2==0?$3:$2)",\"size\":"$1"}"}' >"$f.metadata"

      - name: Upload GeoParquet files to R2
        env:
          RCLONE_CONFIG_DATA: ${{ secrets.RCLONE_CONFIG_DATA }}
        run: |
          d=${{ steps.date.outputs.tag }}
          rclone copyto "planet-$d.osm.parquet"          "$REMOTE_NAME:openplanetdata/osm/planet/geoparquet/planet-latest.osm.parquet"
          rclone copyto "planet-$d.osm.parquet.metadata" "$REMOTE_NAME:openplanetdata/osm/planet/geoparquet/planet-latest.osm.parquet.metadata"
          rclone copyto "planet-$d.osm.parquet.sha256"   "$REMOTE_NAME:openplanetdata/osm/planet/geoparquet/planet-latest.osm.parquet.sha256"
